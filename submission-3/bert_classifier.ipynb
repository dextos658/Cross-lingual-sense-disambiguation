{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert-classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1yV02NQFJ6iO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e3742f7-7b0b-44e7-be27-0b913985639d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "5XhL_nH7J8JG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip3 install torch torchvision\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "fs0wCTjAt7PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "HgpkXt76r-jI"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "from transformers import BertModel, BertForMaskedLM\n",
        "from transformers import AutoModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cosine\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def concaten(token_embeddings):\n",
        "  # concat\n",
        "  token_vecs_cat = []\n",
        "\n",
        "  for token in token_embeddings:\n",
        "      \n",
        "      cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4], token[-5], token[-6]), dim=0)\n",
        "      \n",
        "      token_vecs_cat.append(cat_vec)\n",
        "\n",
        "  # print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))\n",
        "  \n",
        "  return token_vecs_cat"
      ],
      "metadata": {
        "id": "gS_qdy0vzq_D"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summ(token_embeddings):\n",
        "  # sum\n",
        "  token_vecs_sum = []\n",
        "\n",
        "  for token in token_embeddings:\n",
        "    \n",
        "      sum_vec = torch.sum(token[-6:], dim=0)\n",
        "      \n",
        "      token_vecs_sum.append(sum_vec)\n",
        "\n",
        "  # print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
        "\n",
        "  return token_vecs_sum"
      ],
      "metadata": {
        "id": "Wh23HqZG23Hn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def funcyo(text, text2, cat, token):\n",
        "\n",
        "  tokenized_text = tokenizer.tokenize(text, padding=True, truncation=True, return_tensors='pt')\n",
        "  tokenized_text.append('[SEP]')\n",
        "  tokenized_text.insert(0, '[CLS]')\n",
        "\n",
        "  tokenized_text2 = tokenizer.tokenize(text2, padding=True, truncation=True, return_tensors='pt')\n",
        "  tokenized_text = tokenized_text + tokenized_text2\n",
        "\n",
        "  nulls = 1 + tokenized_text.index('[SEP]')\n",
        "\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  segments_ids = [0] * nulls + [1] * (len(tokenized_text) - nulls)\n",
        "\n",
        "\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "  hidden_states = outputs[2]\n",
        "\n",
        "  token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "  token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "  token_embeddings = token_embeddings.permute(1,0,2)\n",
        "  #token_embeddings.size()\n",
        "\n",
        "  if cat:\n",
        "    token_vecs_m = concaten(token_embeddings)\n",
        "  else:\n",
        "    token_vecs_m = summ(token_embeddings)\n",
        "\n",
        "  token_vecs = hidden_states[-2][0]\n",
        "  sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "\n",
        "  diff_bank = 0\n",
        "  a = b = c = d = -1\n",
        "  if token not in tokenized_text or token not in tokenized_text2:\n",
        "    indices = [i for i, x in enumerate(tokenized_text) if x in token]\n",
        "    for i in range(len(indices)):\n",
        "      if i+1 < len(indices) and indices[i+1]-indices[i]==1:\n",
        "        if a == -1:\n",
        "          a = i\n",
        "          b = i+1\n",
        "        else:\n",
        "          c = i\n",
        "          d = i+1\n",
        "    diff_bank1 = 1 - cosine(token_vecs_m[indices[a]], token_vecs_m[indices[c]])\n",
        "    diff_bank2 = 1 - cosine(token_vecs_m[indices[b]], token_vecs_m[indices[d]])\n",
        "    # diff_bank3 = 1 - cosine(token_vecs_m[indices[0]], token_vecs_m[indices[1]])\n",
        "    # diff_bank = (diff_bank1 + diff_bank2 + diff_bank3) / 3\n",
        "    diff_bank = (diff_bank1 + diff_bank2) / 2\n",
        "\n",
        "  else:\n",
        "    indices = [i for i, x in enumerate(tokenized_text) if x == token]\n",
        "    if len(indices) > 1:\n",
        "      diff_bank = 1 - cosine(token_vecs_m[indices[0]], token_vecs_m[indices[1]])\n",
        "    #print('Vector similarity:  %.2f' % diff_bank)\n",
        "\n",
        "  return diff_bank"
      ],
      "metadata": {
        "id": "1hU5lemYnyer"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sheet = \"Stavki anotirani rocno\"\n",
        "wb = gc.open(sheet)\n",
        "rows = wb.sheet1.get_all_values();"
      ],
      "metadata": {
        "id": "4nYOz6q5koMF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"EMBEDDIA/sloberta\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
        "#tokenizer = BertTokenizer.from_pretrained(\"EMBEDDIA/crosloengual-bert\")\n",
        "\n",
        "model = BertModel.from_pretrained(\"EMBEDDIA/sloberta\", output_hidden_states = True)\n",
        "#model = BertModel.from_pretrained('bert-base-multilingual-uncased', output_hidden_states = True)\n",
        "#model = BertModel.from_pretrained('EMBEDDIA/crosloengual-bert', output_hidden_states = True)\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "wo65HweQsGER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wb2 = gc.open('output').sheet1\n",
        "count = 1;\n",
        "\n",
        "for row in rows:\n",
        "\n",
        "  if row[0] != \"\":\n",
        "    token = \"‚ñÅ\" + row[0]\n",
        "\n",
        "    text1 = row[2]\n",
        "    text2 = row[3]\n",
        "    cat = True # change to false to use sum instead\n",
        "    diff = funcyo(text1, text2, cat, token)\n",
        "\n",
        "    cells = wb2.range('A'+str(count)+':K'+str(count))\n",
        "\n",
        "    cells[0].value = str(diff)\n",
        "    #cells[1].value = row[0]\n",
        "    #cells[2].value = \n",
        "    #cells[3].value = row[2]\n",
        "    #cells[4].value = row[3]\n",
        "\n",
        "    wb2.update_cells(cells)\n",
        "\n",
        "    count += 1"
      ],
      "metadata": {
        "id": "3JWjtn2TkrKM"
      },
      "execution_count": 82,
      "outputs": []
    }
  ]
}