{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparing sentences by meaning**"
      ],
      "metadata": {
        "id": "_2dUepkesrdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers"
      ],
      "metadata": {
        "id": "bVRPK8wBZ3xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "v9M4CicjZvwZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer = AutoTokenizer.from_pretrained(\"EMBEDDIA/sloberta\")\n",
        "#model = AutoModel.from_pretrained(\"EMBEDDIA/sloberta\",output_hidden_states=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EMBEDDIA/crosloengual-bert\")\n",
        "model = AutoModel.from_pretrained(\"EMBEDDIA/crosloengual-bert\",output_hidden_states=True)"
      ],
      "metadata": {
        "id": "klKwOTENZ01w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(text,token_length):\n",
        "    tokens=tokenizer(text,max_length=token_length,padding='max_length',truncation=True)\n",
        "    output=model(torch.tensor(tokens.input_ids).unsqueeze(0),\n",
        "                 attention_mask=torch.tensor(tokens.attention_mask).unsqueeze(0)).hidden_states[-1]\n",
        "    return torch.mean(output,axis=1).detach().numpy()"
      ],
      "metadata": {
        "id": "87fxz309kMvq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_similarity(text1,text2,token_length=40):\n",
        "    out1=get_embeddings(text1,token_length=token_length) \n",
        "    out2=get_embeddings(text2,token_length=token_length)\n",
        "\n",
        "    sim1= cosine_similarity(out1,out2)[0][0]\n",
        "\n",
        "    return sim1"
      ],
      "metadata": {
        "id": "MW8_ZFrSj2AY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*We assume that the words and use-case sentences are stored in your Google Drive account.*"
      ],
      "metadata": {
        "id": "z1gYfmyM8xSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "dvMYZwKKnrqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The file with the words and manually annotated phrases is available in our repository, and needs to be saved in the root folder of your Google Drive. \n",
        "An example of such a file is the corpus-annotated.csv file in our repository in the data folder."
      ],
      "metadata": {
        "id": "z70Y8i_Y80Gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sheet = \"Stavki anotirani rocno\"\n",
        "wb = gc.open(sheet)\n",
        "rows = wb.sheet1.get_all_values();"
      ],
      "metadata": {
        "id": "OEdNyHc5nwxu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of the comparison are written to an table, which must be created in the base folder of your Google Drive and named Output. "
      ],
      "metadata": {
        "id": "RCyyLyc-9cPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wb2 = gc.open('output').sheet1\n",
        "count = 1;\n",
        "\n",
        "threshold = 0.53\n",
        "\n",
        "for row in rows:\n",
        "\n",
        "  text1 = row[2]\n",
        "  text2 = row[3]\n",
        "  similarity = calculate_similarity(text1,text2);\n",
        "\n",
        "  cells = wb2.range('A'+str(count)+':F'+str(count))\n",
        "\n",
        "  cells[0].value = row[0]\n",
        "  cells[1].value = str(similarity)\n",
        "  if (similarity > threshold):\n",
        "    cells[2].value = str(1)\n",
        "  else:\n",
        "    cells[2].value = str(0)\n",
        "  cells[3].value = row[1]\n",
        "  cells[4].value = row[2]\n",
        "  cells[5].value = row[3]\n",
        "\n",
        "  wb2.update_cells(cells)\n",
        "\n",
        "  count += 1"
      ],
      "metadata": {
        "id": "2_C2Poo8j4vi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are written in the form:\n",
        "\n",
        "`<token> <similarity> <predicted_similarity> <hand_anotated_similarity> <sentence1> <sentence2>`\n",
        "\n",
        "Manually estimated similarity of 1 means the same meaning, while a value of 0 means different meanings of the use of the word in the sentences.\n",
        "\n",
        "If calculated similarity is higher than provided threshold, the `predicted_similarity` is set to 1."
      ],
      "metadata": {
        "id": "1nf-_9baqY7a"
      }
    }
  ]
}